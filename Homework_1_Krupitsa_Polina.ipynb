{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия (10 баллов).\n",
    "\n",
    "Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1.\n",
    "\n",
    "\n",
    "Крупицина - 9 букв\n",
    "\n",
    "\n",
    "9/4 = 2, остаток 1\n",
    "#### Напишите сюда свой вариант: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0317402001673783e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9394925357888495e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.60530499e-08, -2.75585696e-08, -3.79586097e-08,  1.52826199e-08,\n",
       "        3.02967556e-08,  1.68082686e-08,  2.47757284e-08,  9.31137559e+01,\n",
       "       -1.81654704e-08, -1.02059134e-07,  2.12004461e-08, -4.84129716e-08,\n",
       "       -1.26132040e-08, -3.69658889e-09,  5.48795955e-08,  1.58661322e-08,\n",
       "       -4.45097684e-08,  4.00319939e-08,  5.27769951e-08, -3.71487958e-08,\n",
       "        1.00249014e-07, -4.58265333e-08, -7.79090368e-08, -1.06069161e-08,\n",
       "        4.54922529e-08, -5.91763205e-09, -6.94299825e-08, -1.43219475e-08,\n",
       "       -2.27110498e-08,  1.89848607e-08,  1.59668060e-08,  7.59040698e-08,\n",
       "        3.31719765e+01, -1.32511733e-08,  2.67562575e-08,  1.50675342e-09,\n",
       "       -2.79731301e-08,  7.89513595e-09,  3.84902806e-09, -1.56587288e-08,\n",
       "       -4.72320050e-08, -3.49965779e-08,  2.72624481e-08, -1.01060119e-08,\n",
       "        9.60413480e+01, -3.17252568e-08, -1.51008353e-08,  1.96802629e-08,\n",
       "        6.94397456e+01, -6.22221973e-09,  3.95981766e-08, -2.74145213e-08,\n",
       "       -2.89785754e-08, -4.24995215e-08,  7.13467260e-09, -4.29891579e-08,\n",
       "        3.21407858e+01,  4.73941281e-09,  5.01651171e-08, -6.07526349e-08,\n",
       "       -6.38788703e-08, -1.00874138e-08,  7.16545648e-09, -6.96311348e-08,\n",
       "        7.63042260e+01, -1.90456098e-08, -1.40471567e-08, -4.36312966e-09,\n",
       "        4.08249828e-08,  4.20141697e-08, -6.85636905e-09,  5.94894614e-08,\n",
       "       -2.02729835e-08, -5.81141049e-09,  2.24144304e-08, -1.44016875e-08,\n",
       "        1.92022760e-08,  8.00723643e-08,  3.38151083e-08,  4.15869843e+01,\n",
       "        5.08700288e-08,  3.25579557e-08,  4.52470245e-08,  9.65565818e+01,\n",
       "        3.25870561e+01, -3.05953865e-08, -2.95963614e-08,  8.53893835e-08,\n",
       "       -3.34005297e-08,  8.06875439e+01,  1.51069547e-08, -7.19568384e-08,\n",
       "        6.18237892e-08,  4.81552227e-09, -8.33834145e-08, -5.16143661e-08,\n",
       "       -5.10711606e-08, -1.90740391e-08, -3.26279304e-08,  2.63353954e-09])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n",
    "\n",
    "Разница вызвана тем, что при использовании Градиентного спуска необходимо подбирать гиперпараметры - размер шага, количество итераций, lerning rate. Это связано с тем, что метод градиентного спуска находит в первую очередь локальные минимумы. Поэтому перебор гиперпараметров позволяет найти разные локальные минимумы и выбрать из них самый лучший. Также очень часто градиентный спуск может \"не дойти\" или наоборот \"перешагнуть\" минимум,  с этим также позволяет справиться подбор гиперпараметров.\n",
    "\n",
    "***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.365257477559594e-12\n",
      "4.895776114152901e-14\n",
      "4.949083537438866e-16\n",
      "4.93747841427208e-18\n",
      "5.671671770690672e-20\n",
      "1.6541221563899192e-21\n",
      "9.099347277571536e-25\n",
      "9.56701103388565e-25\n"
     ]
    }
   ],
   "source": [
    "for i in [0.00000001, 0.000000001, 0.0000000001, 0.00000000001, 0.000000000001, 0.0000000000001, 0.00000000000001, 0.000000000000001]:\n",
    "    reg = SGDRegressor(alpha=i).fit(X, y)\n",
    "    print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы добились примерно того же порядка ответа, поскольку числа максимально близки к нулю, то разница не такая уж и большая.. это все по сути 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.01, l_ratio=0.0001, tol=0.00001, max_iter=7000):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        alpha - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "             \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        \n",
    "        #добавляем константный признак и инициализируем веса\n",
    "        \n",
    "        x_0 = np.ones((X.shape[0], 1))\n",
    "        X_new = np.column_stack([x_0, X])\n",
    "        np.random.seed(1234) #фиксим сид чтобы результаты не сбивались от раза к разу\n",
    "        w_init = np.random.uniform(np.min(X_new), np.max(X_new), (X_new.shape[1]))\n",
    "        \n",
    "\n",
    "        \n",
    "        #для хранения весов\n",
    "        self.weights = []\n",
    "        self.weights.append(w_init)\n",
    "        MSE_last = 0 \n",
    "\n",
    "        \n",
    "        for i in range(self.max_iter): #функция остановится на максимуме итераций\n",
    "            y_pred = (X_new).dot(w_init)\n",
    "            w_grad = w_init.copy()\n",
    "            w_grad[-1] = 0\n",
    "            \n",
    "            grad = X_new.transpose().dot(X_new.dot(w_init) - y) * 2 / len(y) + 2 * self.l_ratio * w_grad\n",
    "            w_init = w_init - self.alpha * grad \n",
    "            \n",
    "            MSE = np.sum((y_pred - y) ** 2) / X_new.shape[0] \n",
    "            \n",
    "            if np.abs(MSE - MSE_last) < self.tol:  #еще один критерий остановка если мсе почти перестанет изменяться до конца итераций\n",
    "                self.weights.append(w_init)\n",
    "                break\n",
    "            else:\n",
    "                self.weights.append(w_init)\n",
    "                MSE_last = MSE\n",
    "        \n",
    "        return self.weights\n",
    "        \n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        x_0 = np.ones((X.shape[0], 1))\n",
    "        X_new = np.column_stack([x_0, X])\n",
    "        return np.dot(X_new, self.weights[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009280826600563813"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, my_reg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9726456105506845e-08\n"
     ]
    }
   ],
   "source": [
    "#l2\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "l2_reg = Ridge(alpha = 0.01, tol=0.001, max_iter=1000)\n",
    "l2_reg.fit(X, y)\n",
    "print(mean_squared_error(y, l2_reg.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009876415896989939\n"
     ]
    }
   ],
   "source": [
    "#l1\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "l1_reg = Lasso(alpha = 0.01, tol=0.001, max_iter=1000)\n",
    "l1_reg.fit(X, y)\n",
    "print(mean_squared_error(y, l1_reg.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009280826600563813\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression(alpha = 0.01)\n",
    "my_reg.fit(X, y)\n",
    "print(mean_squared_error(y, my_reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну готовая функция оказалась заметно лучше, хотя по сравнению с l1 регуляризацией l2 явно оказывается более успешной. Я предполагаю, что в написанной функции делается большее количество шагов - у нас все таки есть очень определенный критерий, после которого мы останавливаемся. Можно было бы попробовать сделать больше шагов и посмотреть, прийдем ли мы ближе к минимуму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
